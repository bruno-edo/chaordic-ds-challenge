Chaordic ML Challenge
===================

This repository contains my machine learning solution to the challenge proposed by [Chaordic](https://www.chaordic.com.br/). 

In this challenge we had to find out the **gender** of users, based on their **clickstream** data (in one of the many e-commerces that the company's responsible for), and a **catalog** of products. If you wish to know more about the challenge click [here](https://chaordic.github.io/machinelearning-challenge/).

If you only wish to know how to run the code, and understand what's being done by it, jump to the **Understanding the Code** section of this document.

> **Note:**
>
> The tools and processes described here were performed under a Windows 7 environment. The code should work on Linux, but it's possible that a few lines of code may have to be changed.

----------

### Table of Contents

- [Tools Used](#tools-used)
- [Files](#files)
	- [Notebooks](#notebooks)
	- ["Outputs" folder](#"outputs"-folder)
	- [CSVs](#csvs)
- [Understanding the Code](#understanding-the-code)
	- [Processing the catalog](#processing-the-catalog)
	- [Processing target and training data](#processing-target-and-training-data)
	- [Running the code](#running-the-code)

----------
Tools Used
-------------

For this project the following tools, and libraries, were utilized:

- Jupyter
- Python
	- Scikit-learn
	- Pandas
	- XGBoost*
	- Matplotlib


> **\*** For Windows installation click [here](https://www.ibm.com/developerworks/community/blogs/jfp/entry/Installing_XGBoost_For_Anaconda_on_Windows?lang=en).

----------

Files
-------------------

#### Notebooks

 - **catalog-treatment**: contains the treatment given to the product's catalog;
 - **data-treatment**: contains the treatment given to the training data and the predictive model utilized;
 - **target-treatment**: contains the treatment given to the training data, so it can be used by the predictive model created;
 - **polished-data-treatment**: same as **data-treatment** file, but comments were added and notebook cells that were confusing/unnecessary were deleted, it also leaves out work done towards models that didn't produce the best results;
 - **polished-target-treatment**: same as **target-treatment** file, but comments were added and notebook cells that were confusing/unnecessary were deleted, it also leaves out work done towards models that didn't produce the best results;

#### "Outputs" folder

This folder contain the **outputs** generated by each iteration of the predictive model. Only files generated by runs that were **submitted** for evaluation are contained here.

#### CSVs

 - **processed_catalog**: file produced by running the **catalog-treatment** notebook;
 - **target_final_data**: file produced by running the **polished-target-treatment** notebook;

----------

Understanding the Code
-------------------

> **Important**: If you want to see all the work that was done for this challenge take a look at the files **target-treatment** and **data-treatment**, but bear in mind that those files are very raw and might be confusing. While they do contain the work that resulted in the best model created here, they also contain code related to different approaches to the data modelling and prediction.
>
>If you don't want to see any of that: ignore these files and work with the files that have **polished** in their name.

#### Processing the catalog



#### Processing target and training data

#### Running the code

The notebooks should be ran in the following order:

> **catalog-treatment** --> **polished-target-treatment** --> **polished-data-treatment**

Input files should be placed in a new folder called **inputs**, which should be created in the root directory of this project. Any additional files needed by the notebooks will be generated, and loaded, automatically.

When you finish running the notebooks a CSV file called **predictions** should've been created in the root directory of this project. This CSV will have two columns, one for the **user ID** and one for the predicted **gender**.
